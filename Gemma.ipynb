{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e640520-8c77-4fe8-a718-9e1ceba06bc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install psutil\n",
    "#this is for the RAM\n",
    "!pip install --upgrade pandas\n",
    "import psutil\n",
    "import torch\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import threading\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f90f7f-6902-456c-ab32-c28d168108f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_memory_usage():\n",
    "    available = psutil.virtual_memory().available / (1024 * 1024 * 1024)  # Convert to GB\n",
    "    total = psutil.virtual_memory().total / (1024 * 1024 * 1024)  # Convert to GB\n",
    "    used = psutil.virtual_memory().used / (1024 * 1024 * 1024)  # Convert to GB\n",
    "    print(f\"Total RAM: {total:.2f} GB\")\n",
    "    print(f\"Used RAM: {used:.2f} GB\")\n",
    "    print(f\"Available RAM: {available:.2f} GB\")\n",
    "\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24d22e5-0522-40e9-944d-43b95d659628",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = \"google/gemma-2b\"\n",
    "modelpath=\"model\"\n",
    "model_cache_dir = \"modelCheckpoint\"  # Specify your desired directory here\n",
    "torch.cuda.empty_cache()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,token=\"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\")\n",
    "my_model =AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             cache_dir=modelpath,\n",
    "                                            torch_dtype=torch.float16,\n",
    "                                              device_map={'': 'cuda:0'}, \n",
    "                                               #device_map=\"auto\",\n",
    "                                              token=\"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\") \n",
    "print(f\"Memory footprint: {my_model.get_memory_footprint() / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1278ef-9787-4146-a7c6-7ce42ebd4230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "SC2_INSTRUCT_PROMPT = \"\"\"You are an exceptionally intelligent coding assistant that consistently delivers accurate and reliable responses to user instructions.\n",
    "\n",
    "### Instruction\n",
    "{instruction}\n",
    "\n",
    "### Response\n",
    "{response}\"\"\"\n",
    "def extract_prompts(file_path):\n",
    "    prompts = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            entry = json.loads(line)\n",
    "            task_id = entry.get('task_id', '')  # Assuming each entry has a unique task_id\n",
    "            prompt = entry.get('prompt', '').strip()\n",
    "            \n",
    "            # Adding instructional text and code block formatting\n",
    "            prompt_header = \"Write a Python function to solve the given task:\"\n",
    "            instruction = f\"\"\"{prompt_header}\n",
    "```python\n",
    "{prompt}\n",
    "```\"\"\"\n",
    "\n",
    "            # Define the response prefix template\n",
    "            prefix_template = \"```python\\n{prompt}\"\n",
    "            # Assuming the use of markdown for code blocks, add a newline if needed\n",
    "            prefix = \"\" if SC2_INSTRUCT_PROMPT.endswith(\"\\n\") else \"\\n\"\n",
    "            response_prefix = prefix + (\n",
    "                prefix_template.replace(\"{prompt}\", prompt)\n",
    "                if \"{prompt}\" in prefix_template\n",
    "                else prefix_template\n",
    "            )\n",
    "            \n",
    "            # Add formatted instruction and response prefix instead of raw prompt\n",
    "            prompts[task_id] = {\n",
    "                \"instruction\": instruction,\n",
    "                \"response_prefix\": response_prefix\n",
    "            }\n",
    "    return prompts\n",
    "\n",
    "# Usage example:\n",
    "prompts = extract_prompts(\"humaneval/human-eval-v2-20210705.jsonl\")\n",
    "def extract_function_body(completion: str) -> str:\n",
    "    response_marker = \"### Response\"\n",
    "    code_block_marker = \"```\"\n",
    "\n",
    "    response_start = completion.find(response_marker)\n",
    "    if response_start == -1:\n",
    "        return \"\"\n",
    "\n",
    "    code_block_start = completion.find(code_block_marker, response_start)\n",
    "    if code_block_start == -1:\n",
    "        return \"\"\n",
    "\n",
    "    code_block_start += len(code_block_marker)\n",
    "    code_block_end = completion.find(code_block_marker, code_block_start)\n",
    "    if code_block_end == -1:\n",
    "        code_block_end = len(completion)\n",
    "\n",
    "    function_code = completion[code_block_start:code_block_end].strip()\n",
    "    function_def_start = function_code.find('def ')\n",
    "    if function_def_start == -1:\n",
    "        return \"\"\n",
    "\n",
    "    function_body_start = function_code.find('\\n', function_def_start)\n",
    "    if function_body_start == -1:\n",
    "        return \"\"\n",
    "\n",
    "    function_body = function_code[function_body_start:].strip()\n",
    "    # Remove any docstrings\n",
    "    while '\"\"\"' in function_body or \"'''\" in function_body:\n",
    "        docstring_start = function_body.find('\"\"\"') if '\"\"\"' in function_body else function_body.find(\"'''\")\n",
    "        docstring_end = function_body.find('\"\"\"', docstring_start + 3) if '\"\"\"' in function_body else function_body.find(\"'''\", docstring_start + 3)\n",
    "        if docstring_end == -1:\n",
    "            break\n",
    "        function_body = function_body[:docstring_start] + function_body[docstring_end + 3:]\n",
    "\n",
    "    \n",
    "\n",
    "    return function_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c297c3-a671-4e0b-8b4c-a7475db0fd51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "import threading\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Placeholder for global flag to control the background monitoring\n",
    "keep_monitoring = True\n",
    "\n",
    "def fetch_gpu_power():\n",
    "    \"\"\"Fetch the current power usage of GPUs using nvidia-smi.\"\"\"\n",
    "    # The command to fetch power usage\n",
    "    cmd = \"nvidia-smi --query-gpu=power.draw --format=csv,noheader,nounits\"\n",
    "    power_draw_str = subprocess.check_output(cmd, shell=True).decode('utf-8').strip().split('\\n')\n",
    "    # Convert power draw strings to floats\n",
    "    power_draw = [float(x) for x in power_draw_str]\n",
    "    return power_draw\n",
    "\n",
    "def monitor_gpu_energy_usage(output_csv=\"gpu_energy_usage.csv\"):\n",
    "    # List to store energy readings\n",
    "    readings = []\n",
    "    \n",
    "    while keep_monitoring:\n",
    "        # Fetch real energy readings for the GPUs\n",
    "        gpu_power = fetch_gpu_power()\n",
    "        gpu_0_power = gpu_power[0] if len(gpu_power) > 0 else 0\n",
    "        gpu_1_power = gpu_power[1] if len(gpu_power) > 1 else 0\n",
    "        \n",
    "        # Get the current timestamp\n",
    "        now = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "        \n",
    "        # Append new reading to the list\n",
    "        readings.append([now, gpu_0_power, gpu_1_power])\n",
    "        \n",
    "        # Wait a bit before the next measurement\n",
    "        time.sleep(0.5)  # Adjust the frequency of measurements as needed\n",
    "\n",
    "    # Once monitoring is done, create a DataFrame and save to CSV\n",
    "    df = pd.DataFrame(readings, columns=[\"Timestamp\", \"GPU_0_Power_W\", \"GPU_1_Power_W\"])\n",
    "    # Calculate the total power for each row\n",
    "    df['Total_Power_W'] = df['GPU_0_Power_W'] + df['GPU_1_Power_W']\n",
    "    # Calculate the average power for each row\n",
    "    df['Average_Power_W'] = (df['GPU_0_Power_W'] + df['GPU_1_Power_W']) / 2\n",
    "    # Time interval in hours\n",
    "    time_interval_hours = 0.5 / 3600\n",
    "\n",
    "    # Calculate energy for each GPU in watt-hours (Wh)\n",
    "    df['GPU_0_Energy_Wh'] = df['GPU_0_Power_W'] * time_interval_hours\n",
    "    df['GPU_1_Energy_Wh'] = df['GPU_1_Power_W'] * time_interval_hours\n",
    "    # If you want a running total of energy consumption, you can do a cumulative sum\n",
    "    df['GPU_0_Energy_Wh_Cumulative'] = df['GPU_0_Energy_Wh'].cumsum()\n",
    "    df['GPU_1_Energy_Wh_Cumulative'] = df['GPU_1_Energy_Wh'].cumsum()\n",
    "\n",
    "    # Plot GPU_0 Power\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df['Timestamp'], df['GPU_0_Power_W'], label='GPU 0 Power (W)', marker='o')\n",
    "\n",
    "    # Plot GPU_1 Power\n",
    "    plt.plot(df['Timestamp'], df['GPU_1_Power_W'], label='GPU 1 Power (W)', marker='x')\n",
    "\n",
    "    plt.title('GPU Power Consumption Over Time')\n",
    "    plt.xlabel('Timestamp')\n",
    "    plt.ylabel('Power (W)')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "    plt.tight_layout()  # Adjust layout to fit labels\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting Total Power Consumption in Wh\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df['Timestamp'], df['Total_Power_W'], label='Total Power (W)', marker='o', linestyle='-', color='purple')\n",
    "    plt.title('Total GPU Power Consumption Over Time')\n",
    "    plt.xlabel('Timestamp')\n",
    "    plt.ylabel('Total Power (W)')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()  # Display the second plot\n",
    "\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "def perform_inference(prompts):\n",
    "    \"\"\"Perform model inference on a list of prompts.\"\"\"\n",
    "    results = {}\n",
    "    for task_id, prompt in prompts.items():\n",
    "        input_text = SC2_INSTRUCT_PROMPT.format(\n",
    "            instruction=prompt[\"instruction\"],\n",
    "            response=prompt[\"response_prefix\"]\n",
    "        )\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        input_ids = input_ids.to(\"cuda:0\")\n",
    "        stop_tokens = [\"\\n```\"]\n",
    "        \n",
    "        output = my_model.generate(\n",
    "            input_ids, temperature=0.0, top_p=1, num_return_sequences=1, \n",
    "            do_sample=False, pad_token_id=tokenizer.eos_token_id, \n",
    "            eos_token_id=tokenizer.convert_tokens_to_ids(stop_tokens), max_new_tokens=150\n",
    "        )\n",
    "        \n",
    "        completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        results[task_id] = completion\n",
    "    \n",
    "    return results\n",
    "\n",
    "def extract_and_write_results(results, output_jsonl=\"Gemma/outputGemma2b_test.jsonl\"):\n",
    "    \"\"\"Extract function bodies from results and write to JSONL.\"\"\"\n",
    "    extracted_results = []\n",
    "    for task_id, completion in results.items():\n",
    "        truncated_response = extract_function_body(completion)\n",
    "        extracted_results.append({'task_id': task_id, 'completion': truncated_response})\n",
    "    \n",
    "    # Write to JSONL file\n",
    "    with open(output_jsonl, 'w') as f:\n",
    "        for result in extracted_results:\n",
    "            f.write(json.dumps(result) + '\\n')\n",
    "\n",
    "# Start monitoring in a background thread\n",
    "monitor_thread = threading.Thread(target=monitor_gpu_energy_usage, args=(\"Gemma/gpu_energy_usageGemma2b_16bitTensorstest.csv\",))\n",
    "monitor_thread.start()\n",
    "\n",
    "# From HumanEval the first 10 tasks\n",
    "try:\n",
    "    inference_results = perform_inference(prompts)\n",
    "finally:\n",
    "    # Ensure the monitoring stops when the main task is done\n",
    "    keep_monitoring = False\n",
    "    monitor_thread.join()\n",
    "\n",
    "# Process results after monitoring is complete\n",
    "extract_and_write_results(inference_results)\n",
    "\n",
    "print(\"Monitoring stopped. CSV file should be generated with real GPU power usage values.\")\n",
    "print(\"Inference results have been processed and written to JSONL file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fc1a9b-123e-41a5-80ca-4b638ea693f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_average_total_power(csv_file):\n",
    "    \"\"\"\n",
    "    Function to calculate the average of the 'Total_Power_W' column from a CSV file.\n",
    "    \n",
    "    Args:\n",
    "    csv_file (str): Path to the CSV file.\n",
    "    \n",
    "    Returns:\n",
    "    float: The average of the 'Total_Power_W' column.\n",
    "    \"\"\"\n",
    "    # Load the data from the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Calculate the average of the 'Total_Power_W' column\n",
    "    average_total_power = df['Total_Power_W'].mean()\n",
    "    \n",
    "    return average_total_power\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_average_power_single_GPU(csv_file, gpu_column):\n",
    "    \"\"\"\n",
    "    Function to calculate the average of the specified GPU power column from a CSV file.\n",
    "    \n",
    "    Args:\n",
    "    csv_file (str): Path to the CSV file.\n",
    "    gpu_column (str): The column name for GPU power ('GPU_0_Power_W' or 'GPU_1_Power_W').\n",
    "    \n",
    "    Returns:\n",
    "    float: The average of the specified GPU power column.\n",
    "    \"\"\"\n",
    "    # Load the data from the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Ensure the column exists in the dataframe\n",
    "    if gpu_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{gpu_column}' does not exist in the CSV file.\")\n",
    "    \n",
    "    # Calculate the average of the specified GPU power column\n",
    "    average_power = df[gpu_column].mean()\n",
    "    \n",
    "    return average_power\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fef560-654d-439b-b892-dd80a09d1ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_one_GPU1 = calculate_average_power_single_GPU(\"Gemma/gpu_energy_usageGemma2b_16bitTensors4.csv\",\"GPU_0_Power_W\")\n",
    "gemma_one_GPU2 = calculate_average_power_single_GPU(\"Gemma/gpu_energy_usageGemma2b_16bitTensors5.csv\",\"GPU_1_Power_W\")\n",
    "gemma_one_GPU3 = calculate_average_power_single_GPU(\"Gemma/gpu_energy_usageGemma2b_16bitTensors6.csv\",\"GPU_1_Power_W\")\n",
    "\n",
    "gemma_two_GPU1 = calculate_average_total_power(\"Gemma/gpu_energy_usageGemma2b_16bitTensors.csv\")\n",
    "gemma_two_GPU2 = calculate_average_total_power(\"Gemma/gpu_energy_usageGemma2b_16bitTensors2.csv\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Values for Gemma one GPU and two GPU runs\n",
    "gemma_one_GPU_values1 = [gemma_one_GPU1, gemma_one_GPU2, gemma_one_GPU3]\n",
    "gemma_one_GPU_values2 = [gemma_two_GPU1, gemma_two_GPU2, 0]  # Assuming you only have two runs for two GPU setup\n",
    "\n",
    "# Group labels\n",
    "models = ['Gemma One GPU', 'Gemma Two GPU']\n",
    "values = [gemma_one_GPU_values1, gemma_one_GPU_values2]\n",
    "\n",
    "# Increase the bar width slightly\n",
    "bar_width = 0.3\n",
    "x = np.arange(len(models))\n",
    "offsets = [-bar_width, 0, bar_width]\n",
    "bars_labels = ['Run 1', 'Run 2', 'Run 3']\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(18, 10))  # Adjust the figure size here\n",
    "\n",
    "bars = []\n",
    "for i, (label, offset) in enumerate(zip(bars_labels, offsets)):\n",
    "    bar = ax.bar(x + offset, [values[j][i] for j in range(len(models))], bar_width, label=label)\n",
    "    bars.append(bar)\n",
    "\n",
    "# Adding the values to each bar\n",
    "for bar_group in bars:\n",
    "    for bar in bar_group:\n",
    "        height = bar.get_height()\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            height,\n",
    "            f'{height:.2f}',\n",
    "            ha='center',\n",
    "            va='bottom'\n",
    "        )\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('Total Power Draw')\n",
    "ax.set_title('Total Power Draw for Gemma Models')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "# Placing the legend outside of the plot area\n",
    "ax.legend(title='Runs', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Adjust layout to ensure space for the legend\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "\n",
    "# Display the bar chart\n",
    "plt.show()\n",
    "\n",
    "# Save the bar chart as a PNG file\n",
    "output_filename = \"average_per_GPU_all_models.png\"\n",
    "fig.savefig(output_filename, format='png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2db3bc9-4020-4396-9428-7b8cfdd3913a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_total_time_in_seconds(csv_file_path):\n",
    "    \"\"\"\n",
    "    Calculate the total time difference in seconds between the first and last timestamp in a CSV file.\n",
    "    The CSV must have a column named 'Timestamp' with datetime format '%Y-%m-%d %H:%M:%S'.\n",
    "    \n",
    "    :param csv_file_path: Path to the CSV file\n",
    "    :return: Total time in seconds\n",
    "    \"\"\"\n",
    "    # Read the CSV into a DataFrame and parse the Timestamp column as datetime objects\n",
    "    df = pd.read_csv(csv_file_path, parse_dates=['Timestamp'])\n",
    "    \n",
    "    # Ensure that the DataFrame is sorted by the Timestamp column\n",
    "    df = df.sort_values(by='Timestamp')\n",
    "    \n",
    "    # Get the first and last timestamps\n",
    "    start_time = df['Timestamp'].iloc[0]\n",
    "    end_time = df['Timestamp'].iloc[-1]\n",
    "    \n",
    "    # Calculate the time difference in seconds\n",
    "    time_difference_seconds = (end_time - start_time).total_seconds()\n",
    "    \n",
    "    return time_difference_seconds\n",
    "# Calculate the total time in seconds for each CSV file\n",
    "gemma_one_GPU1_time = calculate_total_time_in_seconds(\"Gemma/gpu_energy_usageGemma2b_16bitTensors4.csv\")\n",
    "gemma_one_GPU2_time = calculate_total_time_in_seconds(\"Gemma/gpu_energy_usageGemma2b_16bitTensors5.csv\")\n",
    "gemma_one_GPU3_time = calculate_total_time_in_seconds(\"Gemma/gpu_energy_usageGemma2b_16bitTensors6.csv\")\n",
    "\n",
    "gemma_two_GPU1_time = calculate_total_time_in_seconds(\"Gemma/gpu_energy_usageGemma2b_16bitTensors.csv\")\n",
    "gemma_two_GPU2_time = calculate_total_time_in_seconds(\"Gemma/gpu_energy_usageGemma2b_16bitTensors2.csv\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Values for Gemma one GPU and two GPU runs (total time in seconds)\n",
    "gemma_one_GPU_times = [gemma_one_GPU1_time, gemma_one_GPU2_time, gemma_one_GPU3_time]\n",
    "gemma_two_GPU_times = [gemma_two_GPU1_time, gemma_two_GPU2_time, 0]  # Assuming you only have two runs for two GPU setup\n",
    "\n",
    "# Group labels\n",
    "models = ['Gemma One GPU', 'Gemma Two GPU']\n",
    "values = [gemma_one_GPU_times, gemma_two_GPU_times]\n",
    "\n",
    "# Increase the bar width slightly\n",
    "bar_width = 0.3\n",
    "x = np.arange(len(models))\n",
    "offsets = [-bar_width, 0, bar_width]\n",
    "bars_labels = ['Run 1', 'Run 2', 'Run 3']\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(18, 10))  # Adjust the figure size here\n",
    "\n",
    "bars = []\n",
    "for i, (label, offset) in enumerate(zip(bars_labels, offsets)):\n",
    "    bar = ax.bar(x + offset, [values[j][i] for j in range(len(models))], bar_width, label=label)\n",
    "    bars.append(bar)\n",
    "\n",
    "# Adding the values to each bar\n",
    "for bar_group in bars:\n",
    "    for bar in bar_group:\n",
    "        height = bar.get_height()\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            height,\n",
    "            f'{height:.2f}',\n",
    "            ha='center',\n",
    "            va='bottom'\n",
    "        )\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('Total Time (seconds)')\n",
    "ax.set_title('Total Time for Gemma Models')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "# Placing the legend outside of the plot area\n",
    "ax.legend(title='Runs', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Adjust layout to ensure space for the legend\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "\n",
    "# Display the bar chart\n",
    "plt.show()\n",
    "\n",
    "# Save the bar chart as a PNG file\n",
    "output_filename = \"total_time_per_GPU_all_models.png\"\n",
    "fig.savefig(output_filename, format='png', dpi=300, bbox_inches='tight')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
