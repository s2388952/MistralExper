{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fd387b-f367-4d6d-b923-99a66dd33bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6ee311-818f-4213-b2c2-4d62bb4ab589",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install psutil\n",
    "#this is for the RAM\n",
    "!pip install --upgrade pandas\n",
    "import psutil\n",
    "import torch\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import threading\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c95352-3ad6-4ba3-a670-32095b793dda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_memory_usage():\n",
    "    available = psutil.virtual_memory().available / (1024 * 1024 * 1024)  # Convert to GB\n",
    "    total = psutil.virtual_memory().total / (1024 * 1024 * 1024)  # Convert to GB\n",
    "    used = psutil.virtual_memory().used / (1024 * 1024 * 1024)  # Convert to GB\n",
    "    print(f\"Total RAM: {total:.2f} GB\")\n",
    "    print(f\"Used RAM: {used:.2f} GB\")\n",
    "    print(f\"Available RAM: {available:.2f} GB\")\n",
    "\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d08ac23-36f7-4772-b7ed-1d35166afca6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(torch.__version__)  \n",
    "print(torch.version.cuda) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5160eb7-1e38-41d5-bd88-780f07c23652",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "modelpath=\"model\"\n",
    "model_cache_dir = \"modelCheckpoint\"  # Specify your desired directory here\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "my_model =AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             cache_dir=modelpath,\n",
    "                                            torch_dtype=torch.float16,\n",
    "                                              device_map=\"auto\",\n",
    "                                              token=\"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\") \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344dece2-f3f8-48d4-8a89-0feefc7d1734",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "SC2_INSTRUCT_PROMPT = \"\"\"You are an exceptionally intelligent coding assistant that consistently delivers accurate and reliable responses to user instructions.\n",
    "\n",
    "### Instruction\n",
    "{instruction}\n",
    "\n",
    "### Response\n",
    "{response}\"\"\"\n",
    "def extract_prompts(file_path):\n",
    "    prompts = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            entry = json.loads(line)\n",
    "            task_id = entry.get('task_id', '')  # Assuming each entry has a unique task_id\n",
    "            prompt = entry.get('prompt', '').strip()\n",
    "            \n",
    "            # Adding instructional text and code block formatting\n",
    "            prompt_header = \"Write a Python function to solve the given task:\"\n",
    "            instruction = f\"\"\"{prompt_header}\n",
    "```python\n",
    "{prompt}\n",
    "```\"\"\"\n",
    "\n",
    "            # Define the response prefix template\n",
    "            prefix_template = \"```python\\n{prompt}\"\n",
    "            # Assuming the use of markdown for code blocks, add a newline if needed\n",
    "            prefix = \"\" if SC2_INSTRUCT_PROMPT.endswith(\"\\n\") else \"\\n\"\n",
    "            response_prefix = prefix + (\n",
    "                prefix_template.replace(\"{prompt}\", prompt)\n",
    "                if \"{prompt}\" in prefix_template\n",
    "                else prefix_template\n",
    "            )\n",
    "            \n",
    "            # Add formatted instruction and response prefix instead of raw prompt\n",
    "            prompts[task_id] = {\n",
    "                \"instruction\": instruction,\n",
    "                \"response_prefix\": response_prefix\n",
    "            }\n",
    "    return prompts\n",
    "\n",
    "# Usage example:\n",
    "prompts = extract_prompts(\"humaneval/human-eval-v2-20210705.jsonl\")\n",
    "print(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0773f205-cf0d-4606-b137-d0db831ebdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_function_body(completion: str) -> str:\n",
    "    response_marker = \"### Response\"\n",
    "    code_block_marker = \"```\"\n",
    "\n",
    "    response_start = completion.find(response_marker)\n",
    "    if response_start == -1:\n",
    "        return \"\"\n",
    "\n",
    "    code_block_start = completion.find(code_block_marker, response_start)\n",
    "    if code_block_start == -1:\n",
    "        return \"\"\n",
    "\n",
    "    code_block_start += len(code_block_marker)\n",
    "    code_block_end = completion.find(code_block_marker, code_block_start)\n",
    "    if code_block_end == -1:\n",
    "        code_block_end = len(completion)\n",
    "\n",
    "    function_code = completion[code_block_start:code_block_end].strip()\n",
    "    function_def_start = function_code.find('def ')\n",
    "    if function_def_start == -1:\n",
    "        return \"\"\n",
    "\n",
    "    function_body_start = function_code.find('\\n', function_def_start)\n",
    "    if function_body_start == -1:\n",
    "        return \"\"\n",
    "\n",
    "    function_body = function_code[function_body_start:].strip()\n",
    "    # Remove any docstrings\n",
    "    while '\"\"\"' in function_body or \"'''\" in function_body:\n",
    "        docstring_start = function_body.find('\"\"\"') if '\"\"\"' in function_body else function_body.find(\"'''\")\n",
    "        docstring_end = function_body.find('\"\"\"', docstring_start + 3) if '\"\"\"' in function_body else function_body.find(\"'''\", docstring_start + 3)\n",
    "        if docstring_end == -1:\n",
    "            break\n",
    "        function_body = function_body[:docstring_start] + function_body[docstring_end + 3:]\n",
    "\n",
    "    \n",
    "\n",
    "    return function_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cf3f1b-31ee-41b2-bbdb-3fa9e0e9c337",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#NEW APPROACH WHERE WRITING TO A FILE OCCURS OUTSIDE OF THE MEASURING\n",
    "import os\n",
    "import warnings\n",
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "import threading\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Placeholder for global flag to control the background monitoring\n",
    "keep_monitoring = True\n",
    "\n",
    "def fetch_gpu_power():\n",
    "    \"\"\"Fetch the current power usage of GPUs using nvidia-smi.\"\"\"\n",
    "    # The command to fetch power usage\n",
    "    cmd = \"nvidia-smi --query-gpu=power.draw --format=csv,noheader,nounits\"\n",
    "    power_draw_str = subprocess.check_output(cmd, shell=True).decode('utf-8').strip().split('\\n')\n",
    "    # Convert power draw strings to floats\n",
    "    power_draw = [float(x) for x in power_draw_str]\n",
    "    return power_draw\n",
    "\n",
    "def monitor_gpu_energy_usage(output_csv=\"gpu_energy_usage.csv\"):\n",
    "    # List to store energy readings\n",
    "    readings = []\n",
    "    \n",
    "    while keep_monitoring:\n",
    "        # Fetch real energy readings for the GPUs\n",
    "        gpu_power = fetch_gpu_power()\n",
    "        gpu_0_power = gpu_power[0] if len(gpu_power) > 0 else 0\n",
    "        gpu_1_power = gpu_power[1] if len(gpu_power) > 1 else 0\n",
    "        \n",
    "        # Get the current timestamp\n",
    "        now = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "        \n",
    "        # Append new reading to the list\n",
    "        readings.append([now, gpu_0_power, gpu_1_power])\n",
    "        \n",
    "        # Wait a bit before the next measurement\n",
    "        time.sleep(0.5)  # Adjust the frequency of measurements as needed\n",
    "\n",
    "    # Once monitoring is done, create a DataFrame and save to CSV\n",
    "    df = pd.DataFrame(readings, columns=[\"Timestamp\", \"GPU_0_Power_W\", \"GPU_1_Power_W\"])\n",
    "    # Calculate the total power for each row\n",
    "    df['Total_Power_W'] = df['GPU_0_Power_W'] + df['GPU_1_Power_W']\n",
    "    # Calculate the average power for each row\n",
    "    df['Average_Power_W'] = (df['GPU_0_Power_W'] + df['GPU_1_Power_W']) / 2\n",
    "    # Time interval in hours\n",
    "    time_interval_hours = 0.5 / 3600\n",
    "\n",
    "    # Calculate energy for each GPU in watt-hours (Wh)\n",
    "    df['GPU_0_Energy_Wh'] = df['GPU_0_Power_W'] * time_interval_hours\n",
    "    df['GPU_1_Energy_Wh'] = df['GPU_1_Power_W'] * time_interval_hours\n",
    "    # If you want a running total of energy consumption, you can do a cumulative sum\n",
    "    df['GPU_0_Energy_Wh_Cumulative'] = df['GPU_0_Energy_Wh'].cumsum()\n",
    "    df['GPU_1_Energy_Wh_Cumulative'] = df['GPU_1_Energy_Wh'].cumsum()\n",
    "\n",
    "    # Plot GPU_0 Power\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df['Timestamp'], df['GPU_0_Power_W'], label='GPU 0 Power (W)', marker='o')\n",
    "\n",
    "    # Plot GPU_1 Power\n",
    "    plt.plot(df['Timestamp'], df['GPU_1_Power_W'], label='GPU 1 Power (W)', marker='x')\n",
    "\n",
    "    plt.title('GPU Power Consumption Over Time')\n",
    "    plt.xlabel('Timestamp')\n",
    "    plt.ylabel('Power (W)')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "    plt.tight_layout()  # Adjust layout to fit labels\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting Total Power Consumption in Wh\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df['Timestamp'], df['Total_Power_W'], label='Total Power (W)', marker='o', linestyle='-', color='purple')\n",
    "    plt.title('Total GPU Power Consumption Over Time')\n",
    "    plt.xlabel('Timestamp')\n",
    "    plt.ylabel('Total Power (W)')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()  # Display the second plot\n",
    "\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "def perform_inference(prompts):\n",
    "    \"\"\"Perform model inference on a list of prompts.\"\"\"\n",
    "    results = {}\n",
    "    for task_id, prompt in prompts.items():\n",
    "        input_text = SC2_INSTRUCT_PROMPT.format(\n",
    "            instruction=prompt[\"instruction\"],\n",
    "            response=prompt[\"response_prefix\"]\n",
    "        )\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        input_ids = input_ids.to(\"cuda\")\n",
    "        stop_tokens = [\"\\n```\"]\n",
    "        \n",
    "        # Convert stop_tokens to IDs\n",
    "        #THE CONFIG FOR END TOKENS IS SLIGHTLY DIFFERENT HERE WITH THE REST OF THE MODEL SETUPS\n",
    "        eos_token_ids = tokenizer.convert_tokens_to_ids(stop_tokens)\n",
    "        \n",
    "        # Ensure eos_token_ids is an integer\n",
    "        if isinstance(eos_token_ids, list) and len(eos_token_ids) == 1:\n",
    "            eos_token_id = eos_token_ids[0]\n",
    "        else:\n",
    "            raise ValueError(\"Failed to convert stop tokens to valid token IDs\")\n",
    "        \n",
    "        output = my_model.generate(\n",
    "            input_ids, temperature=0.0, top_p=1, num_return_sequences=1, \n",
    "            do_sample=False, pad_token_id=tokenizer.eos_token_id, \n",
    "            eos_token_id=eos_token_id, max_new_tokens=150\n",
    "        )\n",
    "        \n",
    "        completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        results[task_id] = completion\n",
    "    \n",
    "    return results\n",
    "\n",
    "def extract_and_write_results(results, output_jsonl=\"outputLlama3_8b_4.jsonl\"):\n",
    "    \"\"\"Extract function bodies from results and write to JSONL.\"\"\"\n",
    "    extracted_results = []\n",
    "    for task_id, completion in results.items():\n",
    "        truncated_response = extract_function_body(completion)\n",
    "        extracted_results.append({'task_id': task_id, 'completion': truncated_response})\n",
    "    \n",
    "    # Write to JSONL file\n",
    "    with open(output_jsonl, 'w') as f:\n",
    "        for result in extracted_results:\n",
    "            f.write(json.dumps(result) + '\\n')\n",
    "\n",
    "# Start monitoring in a background thread\n",
    "monitor_thread = threading.Thread(target=monitor_gpu_energy_usage, args=(\"Llama3/gpu_energy_usageLlama3_8b_16bitTensors7.csv\",))\n",
    "monitor_thread.start()\n",
    "\n",
    "# From HumanEval the first 10 tasks\n",
    "try:\n",
    "    inference_results = perform_inference(prompts)\n",
    "finally:\n",
    "    # Ensure the monitoring stops when the main task is done\n",
    "    keep_monitoring = False\n",
    "    monitor_thread.join()\n",
    "\n",
    "# Process results after monitoring is complete\n",
    "extract_and_write_results(inference_results)\n",
    "\n",
    "print(\"Monitoring stopped. CSV file should be generated with real GPU power usage values.\")\n",
    "print(\"Inference results have been processed and written to JSONL file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87fefa2-0446-4178-a741-18b68c8bfdd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(inference_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d19c93-f495-4e5b-ae7f-be86d0d2ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "##OLD APPROACH WHERE WIRITING TO THE FILE IS INSIDE\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Placeholder for global flag to control the background monitoring\n",
    "keep_monitoring = True\n",
    "def write_jsonl(data, path):\n",
    "    with open(path, 'a') as f:\n",
    "        f.write(json.dumps(data) + '\\n')\n",
    "def fetch_gpu_power():\n",
    "    \"\"\"Fetch the current power usage of GPUs using nvidia-smi.\"\"\"\n",
    "    # The command to fetch power usage\n",
    "    cmd = \"nvidia-smi --query-gpu=power.draw --format=csv,noheader,nounits\"\n",
    "    power_draw_str = subprocess.check_output(cmd, shell=True).decode('utf-8').strip().split('\\n')\n",
    "    # Convert power draw strings to floats\n",
    "    power_draw = [float(x) for x in power_draw_str]\n",
    "    return power_draw\n",
    "\n",
    "def monitor_gpu_energy_usage(output_csv=\"gpu_energy_usage.csv\"):\n",
    "    # List to store energy readings\n",
    "    readings = []\n",
    "    \n",
    "    while keep_monitoring:\n",
    "        # Fetch real energy readings for the GPUs\n",
    "        gpu_power = fetch_gpu_power()\n",
    "        gpu_0_power = gpu_power[0] if len(gpu_power) > 0 else 0\n",
    "        gpu_1_power = gpu_power[1] if len(gpu_power) > 1 else 0\n",
    "        \n",
    "        # Get the current timestamp\n",
    "        now = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "        \n",
    "        # Append new reading to the list\n",
    "        readings.append([now, gpu_0_power, gpu_1_power])\n",
    "        \n",
    "        # Wait a bit before the next measurement\n",
    "        time.sleep(0.5)  # Adjust the frequency of measurements as needed\n",
    "\n",
    "    # Once monitoring is done, create a DataFrame and save to CSV\n",
    "    df = pd.DataFrame(readings, columns=[\"Timestamp\", \"GPU_0_Power_W\", \"GPU_1_Power_W\"])\n",
    "    # Calculate the total power for each row\n",
    "    df['Total_Power_W'] = df['GPU_0_Power_W'] + df['GPU_1_Power_W']\n",
    "    # Calculate the average power for each row\n",
    "    df['Average_Power_W'] = (df['GPU_0_Power_W'] + df['GPU_1_Power_W']) / 2\n",
    "    # Time interval in hours\n",
    "    time_interval_hours = 0.5 / 3600\n",
    "\n",
    "    # Calculate energy for each GPU in watt-hours (Wh)\n",
    "    df['GPU_0_Energy_Wh'] = df['GPU_0_Power_W'] * time_interval_hours\n",
    "    df['GPU_1_Energy_Wh'] = df['GPU_1_Power_W'] * time_interval_hours\n",
    "    # If you want a running total of energy consumption, you can do a cumulative sum\n",
    "    df['GPU_0_Energy_Wh_Cumulative'] = df['GPU_0_Energy_Wh'].cumsum()\n",
    "    df['GPU_1_Energy_Wh_Cumulative'] = df['GPU_1_Energy_Wh'].cumsum()\n",
    "\n",
    "   \n",
    "    # Plot GPU_0 Power\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df['Timestamp'], df['GPU_0_Power_W'], label='GPU 0 Power (W)', marker='o')\n",
    "\n",
    "    # Plot GPU_1 Power\n",
    "    plt.plot(df['Timestamp'], df['GPU_1_Power_W'], label='GPU 1 Power (W)', marker='x')\n",
    "\n",
    "    \n",
    "    plt.title('GPU Power Consumption Over Time')\n",
    "    plt.xlabel('Timestamp')\n",
    "    plt.ylabel('Power (W)')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "    plt.tight_layout()  # Adjust layout to fit labels\n",
    "    plt.show()\n",
    "    # Plotting Total Power Consumption in Wh\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df['Timestamp'], df['Total_Power_W'], label='Total Power (W)', marker='o', linestyle='-', color='purple')\n",
    "    plt.title('Total GPU Power Consumption Over Time')\n",
    "    plt.xlabel('Timestamp')\n",
    "    plt.ylabel('Total Power (W)')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()  # Display the second plot\n",
    "    df.to_csv(output_csv, index=False)\n",
    "def perform_inference(prompts):\n",
    "    \"\"\"Perform model inference on a list of prompts.\"\"\"\n",
    "    for task_id, prompt in prompts.items():\n",
    "        input_text = SC2_INSTRUCT_PROMPT.format(\n",
    "            instruction=prompt[\"instruction\"],\n",
    "            response=prompt[\"response_prefix\"]\n",
    "        )\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        input_ids = input_ids.to(\"cuda\")\n",
    "        stop_tokens = [\"\\n```\"]\n",
    "        \n",
    "        # Convert stop_tokens to IDs\n",
    "        eos_token_ids = tokenizer.convert_tokens_to_ids(stop_tokens)\n",
    "        \n",
    "        # Ensure eos_token_ids is an integer\n",
    "        if isinstance(eos_token_ids, list) and len(eos_token_ids) == 1:\n",
    "            eos_token_id = eos_token_ids[0]\n",
    "        else:\n",
    "            raise ValueError(\"Failed to convert stop tokens to valid token IDs\")\n",
    "        output = my_model.generate(\n",
    "            input_ids, temperature=0.0, top_p=1, num_return_sequences=1, \n",
    "            do_sample=False, pad_token_id=tokenizer.eos_token_id, \n",
    "            eos_token_id=eos_token_id, max_new_tokens=150\n",
    "        )\n",
    "        completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        truncated_response= extract_function_body(completion)\n",
    "   \n",
    "        write_jsonl({'task_id': task_id, 'completion': truncated_response}, \"Llama3/outputLlama3_8b_3.jsonl\")\n",
    "# Start monitoring in a background thread\n",
    "monitor_thread = threading.Thread(target=monitor_gpu_energy_usage, args=(\"Llama3/gpu_energy_usageLlama3_8b_16bitTensors6.csv\",))\n",
    "monitor_thread.start()\n",
    "\n",
    "# From HumanEval the first 10 tasks\n",
    "try:\n",
    "    perform_inference(prompts)\n",
    "finally:\n",
    "    # Ensure the monitoring stops when the main task is done\n",
    "    keep_monitoring = False\n",
    "    monitor_thread.join()\n",
    "\n",
    "print(\"Monitoring stopped. CSV file should be generated with real GPU power usage values.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
